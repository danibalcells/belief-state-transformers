# Probing Belief States in Transformer Models

An [ARENA 7.0](https://www.arena.education/) capstone replicating and extending [Shai et al. (2024)](https://arxiv.org/abs/2405.15943): *Transformers Represent Belief State Geometry in their Residual Stream*. In collaboration with Marta Emili García Segura and with very helpful feedback from Xavier Poncini (Simplex).

**Core question:** Do transformer language models encode belief states — probability distributions over hidden states of the generating process — in their residual stream?

---

## Overview

We trained a small transformer on sequences generated by a Hidden Markov Model (HMM) and investigated whether its residual stream encodes the optimal Bayesian belief state at each position. The project has two parts:

1. **Replication:** Train a transformer on HMM sequences, extract residual stream activations, and train a linear probe to recover the belief geometry.
2. **Extension:** Use an autoencoder to build an *invertible* mapping between belief space and residual stream space, then causally intervene by injecting new beliefs and measuring how next-token predictions shift.

---

## Background

### The Mess3 HMM

The generating process is **Mess3**, a 3-state, 3-observation HMM from the paper. The model emits tokens `{A, B, C}` according to observation-conditioned transition matrices. The hidden state is never observed.

The **belief state** at each position is the Bayesian posterior over hidden states given all tokens seen so far — a probability vector in the 3-simplex Δ². As more tokens are observed, belief states trace out a self-similar fractal geometry inside the simplex.

`beliefs_by_position_dataset_1000000.png` shows this geometry emerging across sequence positions:

![Belief states by position](images/beliefs_by_position_dataset_1000000.png)

---

## Part 1: Replication

### Transformer

`BeliefStateTransformer` (`transformer.py`) wraps TransformerLens's `HookedTransformer` with the architecture from the paper:

| Hyperparameter | Value |
|---|---|
| Layers | 4 |
| d_model | 64 |
| Heads | 1 |
| d_head | 8 |
| d_mlp | 256 |
| Activation | ReLU |
| Context length | 10 |

Training is via next-token prediction with cross-entropy loss. Both AdamW and SGD runs are saved in `final_checkpoints/`.

`scripts/train_transformer.py` logs loss, accuracy, and KL divergence from the optimal Bayesian predictor to W&B. The PCA plots below show how the residual stream's belief geometry sharpens over the course of AdamW training:

![Residual stream PCA across training steps](images/adamw_pca_multi.png)

### Linear Probe

`scripts/sample_acts.py` runs the trained transformer on HMM sequences and saves `(activation, belief)` pairs to `outputs/datasets/<run_id>/dataset.pt`.

`scripts/train_probe.py` fits a `LinearProbe` (`probes/linear.py`) on these pairs with MSE loss, mapping residual stream activations to the 3D belief vector.

The comparison below shows optimal Bayesian beliefs (left) vs. probe-predicted beliefs (right), both projected onto the 2D simplex:

![Optimal vs. probe-predicted beliefs (2D)](images/comparison-20260204_182437.png)

The probe recovers the fractal simplex geometry from the residual stream with high fidelity. Four 3D views of the probe outputs in the full probability simplex:

![Probe outputs — four 3D views](images/probe_3d_views-20260203_121403.png)

---

## Part 2: Extension — Causal Interventions

### Motivation

A linear probe shows *correlation* between residual stream activations and belief states, but not *causation*. To test whether the encoded belief geometry is actually *used* for next-token prediction, we needed to intervene: swap out the current belief for a new one and check if predictions update accordingly.

Naive subtraction of probe directions is problematic because the 3D belief geometry is non-orthogonal. We instead train an **autoencoder** to learn an invertible mapping.

### Autoencoder

`probes/autoencoder.py` implements an `Autoencoder` with:
- **Encoder:** residual stream (d_model=64) → 2D latent space
- **Decoder:** 2D latent → residual stream

Training (`scripts/train_autoencoder.py`) minimises a combined loss:

```
L = λ_recon · MSE(decoded, activation) + λ_geom · MSE(encoded, projected_belief)
```

The geometry loss `λ_geom` pushes the 2D latent space to match the 2D simplex projection of the optimal belief state; the reconstruction loss `λ_recon` ensures the decoder can invert back to the residual stream. This gives us `decode_from_belief(belief)` — a way to obtain the activation that *would* correspond to any arbitrary belief.

### Steering

`interventions/steering.py` implements two intervention modes:

- **Replace (`SteeringIntervention`):** Fully replace the residual stream activation at a chosen position with the decoded target belief.
- **Additive (`AdditiveSteeringIntervention`):** Blend the original activation with the target activation via a mixing parameter λ ∈ [0, 1].

Three types of injected beliefs are tested:

| Belief source | Description |
|---|---|
| `counterfactual` | What the belief *would* be if the previous token had been different |
| `other_seq_reachable` | A valid belief reachable from some other sequence |
| `random_simplex` | A belief sampled uniformly at random from Δ² |

---

## Results

After steering, we measure:
- **KL(optimal actual ‖ steered pred):** how much the prediction deviates from what's optimal *given the real sequence*
- **KL(optimal injected ‖ steered pred):** how much the prediction deviates from what's optimal *given the injected belief*

If the belief geometry is causally used, the second KL should *decrease* after steering while the first should *increase*.

### Counterfactual steering

![Steering to counterfactual beliefs — summary](images/steering_summary_steering_20260206_113903.png)

The crossover pattern is clear: after injecting a counterfactual belief, predictions shift away from what was optimal for the original sequence and towards what is optimal for the counterfactual. This strongly suggests the encoded belief state is causally upstream of next-token prediction.

### Additive steering sweep

The additive sweep (`scripts/run_additive_steering_sweep.py`) varies λ from 0 to 1, showing a smooth trade-off:

![Additive steering sweep — KL vs. lambda](images/additive_sweep_20260205_113420_unique.png)

---

## Repository Structure

```
.
├── HMM.py                         # Mess3 HMM: sampling, belief computation, optimal predictions
├── transformer.py                 # BeliefStateTransformer (TransformerLens wrapper)
├── probes/
│   ├── base.py                    # Base probe + SteerableProbe interface
│   ├── linear.py                  # LinearProbe (residual stream → belief)
│   ├── autoencoder.py             # Autoencoder (residual stream ↔ 2D belief latent)
│   └── vae.py                     # Variational autoencoder variant
├── interventions/
│   ├── base.py                    # BaseIntervention (loads model + HMM)
│   ├── steering.py                # Replace- and additive-steering interventions
│   └── zero_ablation.py          # Zero-ablation baseline
├── scripts/
│   ├── train_transformer.py       # Train transformer on Mess3 sequences
│   ├── sample_acts.py             # Extract (activation, belief) pairs from trained model
│   ├── train_probe.py             # Train linear probe on saved dataset
│   ├── train_autoencoder.py       # Train autoencoder with geometry + reconstruction loss
│   ├── train_vae.py               # Train VAE variant
│   ├── run_steering_experiment.py # Run replace/additive steering and plot results
│   ├── run_additive_steering_sweep.py  # Sweep lambda for additive steering
│   ├── run_zero_ablation_sweep.py # Zero-ablation sweep
│   ├── plot_belief_probe_comparison.py # Optimal vs. probe beliefs (2D simplex)
│   ├── plot_probe_3d_four_views.py     # Four 3D views of probe output
│   ├── plot_beliefs_by_position.py     # Simplex fill by sequence position
│   ├── plot_steering_from_results.py   # KL plot from saved steering results
│   ├── plot_steering_summary.py        # Summary KL plot (mean + CI)
│   ├── plot_additive_steering_from_results.py
│   ├── plot_additive_steering_sweep.py
│   ├── inspect_steering_results.py
│   └── check_probe_rank.py
├── utils/
│   ├── plotting.py                # Shared plotting utilities
│   └── simplex.py                 # Simplex projection helpers
├── notebooks/
│   └── plot_probe_3d.ipynb        # Interactive 3D probe visualisation
├── tests/
│   ├── test_hmm.py
│   └── test_transformer.py
├── final_checkpoints/             # Saved transformer checkpoints (adamw + sgd)
└── images/                        # Generated figures
```

---

## Setup

```bash
uv venv -p python3.11
source .venv/bin/activate
uv sync
```

## Pipeline

```bash
# 1. Train transformer
python scripts/train_transformer.py --optimizer adamw --epochs 1000000 \
  --save-path final_checkpoints/transformer/adamw/step_1000000.pt

# 2. Extract activations
python scripts/sample_acts.py --checkpoint final_checkpoints/transformer/adamw/step_1000000.pt \
  --num-sequences 100000

# 3. Train linear probe
python scripts/train_probe.py --dataset outputs/datasets/<run_id>/dataset.pt

# 4. Train autoencoder
python scripts/train_autoencoder.py --dataset-path outputs/datasets/<run_id>/dataset.pt

# 5. Run steering experiment
python scripts/run_steering_experiment.py \
  --model-checkpoint final_checkpoints/transformer/adamw/step_1000000.pt \
  --steerable-type autoencoder \
  --steerable-checkpoint outputs/autoencoders/<run_id>/autoencoder.pt \
  --belief-source counterfactual
```

All training runs log to [Weights & Biases](https://wandb.ai) under the `belief-state-transformers` project.

---

## References

- Shai, A. S., Riechers, P. M., Kirsch, L., & Geshkovski, B. (2024). *Transformers Represent Belief State Geometry in their Residual Stream.* [arXiv:2405.15943](https://arxiv.org/abs/2405.15943)
